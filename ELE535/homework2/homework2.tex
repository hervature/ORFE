\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{datetime}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage[english]{babel}
\usepackage{matlab-prettifier}
\usepackage{graphicx}
\usepackage[makeroom]{cancel}
\usepackage{afterpage}
\usepackage{capt-of}
\usepackage{bm}
\usepackage{float}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\newtheoremstyle{colon}{\topsep}{\topsep}{}{}{\bfseries}{:}{ }{}
\theoremstyle{colon}
\newtheorem{exercise}{Exercise}
\newtheorem*{answer}{Answer}

\title{ELE 535: Machine Learning and Pattern Recognition \\ Homework 2}
\author{Zachary Hervieux-Moore}

\newdate{date}{01}{10}{2018}
\date{\displaydate{date}}

\begin{document}

\maketitle

\clearpage

\begin{exercise}
  Let $u \in \mathbb{R}^m$, $v \in \mathbb{R}^n$, and $A \in \mathbb{R}^{m \times n}$. Find the orthogonal projection of $A$ onto span($u v^T$).
\end{exercise}

\begin{answer}
  This is equivalent to projecting onto a line and the proof from class is replicated here.

  \begin{align*}
    &\quad \min_{B \in \mathbb{R}^{m \times n}} \frac{1}{2} \lVert A - B \rVert^2 \\
    &\quad \text{s.t. } B \in \text{span}(u v^T) \\
    &\Longleftrightarrow \alpha^* = \argmin_{\alpha \in \mathbb{R}} \frac{1}{2} \lVert A - \alpha u v^T \rVert^2 \\
    &\Longleftrightarrow \alpha^* = \argmin_{\alpha \in \mathbb{R}} \frac{1}{2} \lVert A \rVert^2 - \alpha \langle A, u v^T \rangle + \frac{1}{2} \lVert u v^T \rVert^2
  \end{align*}

  First order necessary condition and convexity yields a minimum of

  \begin{gather*}
    \alpha^* = \frac{\langle A, u v^T \rangle}{\lVert u v^T \rVert^2}
  \end{gather*}
  Which gives the projection as

  \begin{gather*}
    B = \frac{\langle A, u v^T \rangle}{\lVert u v^T \rVert^2} u v^T
  \end{gather*}

  Notice that the norm could be any one of the valid matrix norms.
\end{answer}

\clearpage

\begin{exercise}
  \textbf{Norm Invariance under Orthogonal Transformations.} Show that for any $A \in \mathbb{R}^{m \times n}$, $Q \in \mathcal{O}_m$, $R \in \mathcal{O}_n$, $\lVert QAR \rVert_F = \lVert A \rVert_F$. Thus the Frovenius norm is invariant under orthogonal transformations. Similarly, show the induced 2-norm of $A \in \mathbb{R}^{m \times n}$ is invariant under orthogonal transformations.
\end{exercise}

\begin{answer}
  First, we show invariance for the Frobenius norm.

  \begin{align*}
    \lVert QAR \rVert_F &= \text{trace}(QAR(QAR)^T) \\
    &= \text{trace}(QARR^TA^TQ^T) \\
    &= \text{trace}(QAA^TQ^T) \\
    &= \text{trace}(AA^TQ^TQ) \\
    &= \text{trace}(AA^T) \\
    &= \lVert A \rVert_F
  \end{align*}

  Now for the induced 2-norm.

  \begin{align*}
    \lVert QAR \rVert_2 &= \sqrt{\langle QAR, QAR \rangle} \\
    &= \sqrt{\langle AR, Q^TQAR \rangle} \\
    &= \sqrt{\langle AR, AR \rangle} \\
    &= \lVert AR \rVert_2 \\
    &= \max_{\lVert x \rVert_2 = 1} \lVert AR x \rVert_2 \\
    &= \max_{\lVert y \rVert_2 = 1} \lVert Ay \rVert_2 \\
    &= \lVert A \rVert_2
  \end{align*}

  Where the second last step of $y = Rx$ is justified because $R$ is orthogonal and so if $\lVert x \rVert_2 = 1$ then $\lVert R x \rVert_2 = 1$. As it is invertible, $x$ can always be retrieved with $x = R^T y$.
\end{answer}

\clearpage

\begin{exercise}
  Let $A, B$ be matrices of appropriate size and $x \in \mathbb{R}^n$. Prove that

  \begin{enumerate}[label=\alph*)]
    \item $\lVert A x \rVert_2 \leq \lVert A \rVert_2 \lVert x \rVert_2$ ;
    \item $\lVert A B \rVert_2 \leq \lVert A \rVert_2 \lVert B \rVert_2$.
  \end{enumerate}
\end{exercise}

\begin{answer}
  \

  \begin{enumerate}[label=\alph*)]
    \item This is direct from the definition.
      \begin{align*}
        &\quad \lVert A \rVert_2 = \max_{x \neq 0} \frac{\lVert A x \rVert_2}{\lVert x \rVert_x} \\
        &\Longleftrightarrow \lVert A \rVert_2 \geq \frac{\lVert A x \rVert_2}{\lVert x \rVert_2} \quad \text{ if } x \neq 0 \\
        &\Longleftrightarrow \lVert A \rVert_2 \lVert x \rVert_2 \geq \lVert A x \rVert_2 \quad \text{ if } x \neq 0
      \end{align*}

      Now, if $x = 0$, the inequality is trivially satisfied.

    \item We simply use part a) twice
      \begin{align*}
        \lVert AB \rVert_2 &= \max_{\lVert x \rVert_2 = 1} \lVert A Bx \rVert_2 \\
        &\leq \max_{\lVert x \rVert_2 = 1} \lVert A \rVert_2 \lVert Bx \rVert_2 \\
        &\leq \max_{\lVert x \rVert_2 = 1} \lVert A \rVert_2 \lVert B \rVert_2 \lVert x \rVert_2 \\
        &= \lVert A \rVert_2 \lVert B \rVert_2
      \end{align*}
  \end{enumerate}
\end{answer}

\clearpage

\begin{exercise}
  For $A, B \in \mathbb{R}^{m \times n}$. Show that $\sigma_1 (A + B) \leq \sigma_1 (A) + \sigma_1 (B)$.
\end{exercise}

\begin{answer}
   Using the fact that $\sigma(A)_1 = \lVert A \rVert_2$. We then apply the definition of the induced 2-norm and use the triangle inequality for vectors

   \begin{align*}
    \lVert A + B \rVert_2 &= \max_{\lVert x \rVert_2} \lVert (A+B)x \rVert_2 \\
    &\leq \max_{\lVert x \rVert_2} \lVert Ax \rVert_2 + \lVert Bx \rVert_2 \\
    &\leq \max_{\lVert x \rVert_2} \lVert Ax \rVert_2 + \max_{\lVert y \rVert_2} \lVert By \rVert_2 \\
    &= \lVert A \rVert_2 + \lVert B \rVert_2
   \end{align*}
\end{answer}

\clearpage

\begin{exercise}
  \textbf{The Moore-Penrose pseudo-inverse.} The Moore-Penrose pseudo-inverse of a matrix $A \in \mathbb{R}^{m \times n}$ is the unique matrix $A^+ \in \mathbb{R}^{n \times m}$ satisfying the following four properties:

  \begin{enumerate}[label=\alph*)]
    \item $A(A^+ A) = A$
    \item $(A^+ A)A^+ = A^+$
    \item $(A^+ A)^T = A^+ A$
    \item $(A A^+)^T = A A^+$
  \end{enumerate}

  Let $A$ have compact SVD $A = U \Sigma V^T$. Show that $A^+ = V \Sigma^{-1} U^T$. Give an interpretation of $A^+$ in terms of $\mathcal{N}(A)$, $\mathcal{N}(A)^\perp$, and $\mathcal{R}(A)$.
\end{exercise}

\begin{answer}
  We simply verify the four properties for $A^+$.

  \begin{enumerate}[label=\alph*)]
    \item
      \begin{gather*}
        A(A^+ A) = U \Sigma V^T (V \Sigma^{-1} U^T U \Sigma V^T) = U \Sigma V^T = A
      \end{gather*}
    \item
      \begin{gather*}
        (A^+ A)A^+ = (V \Sigma^{-1} U^T U \Sigma V^T) V \Sigma^{-1} U^T = V \Sigma^{-1} U^T = A^+
      \end{gather*}
    \item
      \begin{gather*}
        (A^+ A)^T = (V \Sigma^{-1} U^T U \Sigma V^T)^T = I^T = I = V \Sigma^{-1} U^T U \Sigma V^T = A^+ A
      \end{gather*}
    \item
      \begin{gather*}
        (A A^+)^T = (U \Sigma V^T V \Sigma^{-1} U^T)^T = I^T = I = U \Sigma V^T V \Sigma^{-1} U^T = A A^+
      \end{gather*}
  \end{enumerate}

  Note that $A^T = V \Sigma U^T$, thus we have that $\mathcal{R}(A^T) = \mathcal{R}(A^+)$. Using the fact that $\mathcal{R}(A^T) = \mathcal{N}(A)^\perp$. So, $\mathcal{R}(A^+) = \mathcal{N}(A)^\perp$. Also, since $\Sigma$ and $\Sigma^{-1}$ are diagonal, then we have $\mathcal{N}(A^T) = \mathcal{N}(A^+)$ or $\mathcal{R}(A)^\perp = \mathcal{N}(A^+)$. Thus, we get the interpretation:

  \begin{gather*}
    \mathcal{N}(A) \oplus \mathcal{R}(A^+) = \mathbb{R}^n \\
    \mathcal{R}(A) \oplus \mathcal{N}(A^+) = \mathbb{R}^m
  \end{gather*}
\end{answer}

\end{document}