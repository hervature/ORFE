\begin{thebibliography}{10}

\bibitem{silver_mastering_2016}
D.~Silver, A.~Huang, C.~J. Maddison, A.~Guez, L.~Sifre, G.~van~den Driessche,
  J.~Schrittwieser, I.~Antonoglou, V.~Panneershelvam, M.~Lanctot, S.~Dieleman,
  D.~Grewe, J.~Nham, N.~Kalchbrenner, I.~Sutskever, T.~Lillicrap, M.~Leach,
  K.~Kavukcuoglu, T.~Graepel, and D.~Hassabis, ``Mastering the game of go with
  deep neural networks and tree search,'' vol.~529, p.~484.

\bibitem{silver_mastering_2017}
D.~Silver, J.~Schrittwieser, K.~Simonyan, I.~Antonoglou, A.~Huang, A.~Guez,
  T.~Hubert, L.~Baker, M.~Lai, A.~Bolton, Y.~Chen, T.~Lillicrap, F.~Hui,
  L.~Sifre, G.~van~den Driessche, T.~Graepel, and D.~Hassabis, ``Mastering the
  game of go without human knowledge,'' vol.~550, p.~354.

\bibitem{silver_mastering_2017-1}
D.~Silver, T.~Hubert, J.~Schrittwieser, I.~Antonoglou, M.~Lai, A.~Guez,
  M.~Lanctot, L.~Sifre, D.~Kumaran, T.~Graepel, T.~Lillicrap, K.~Simonyan, and
  D.~Hassabis, ``Mastering chess and shogi by self-play with a general
  reinforcement learning algorithm,''

\bibitem{tromp_sequence_2016}
J.~Tromp, ``Sequence a094777.''

\bibitem{auer_finite-time_2002}
P.~Auer, N.~Cesa-Bianchi, and P.~Fischer, ``Finite-time analysis of the
  multiarmed bandit problem,'' vol.~47, no.~2, pp.~235--256.

\bibitem{kocsis_bandit_2006}
L.~Kocsis and C.~Szepesv√°ri, ``Bandit based monte-carlo planning,'' in {\em
  Proceedings of the 17th European Conference on Machine Learning}, {ECML}'06,
  pp.~282--293, Springer-Verlag.

\bibitem{tesauro_-line_1997}
G.~Tesauro and G.~R. Galperin, ``On-line policy improvement using monte-carlo
  search,'' in {\em Advances in Neural Information Processing Systems 9} (M.~C.
  Mozer, M.~I. Jordan, and T.~Petsche, eds.), pp.~1068--1074, {MIT} Press.

\bibitem{billings_challenge_2002}
D.~Billings, A.~Davidson, J.~Schaeffer, and D.~Szafron, ``The challenge of
  poker,'' vol.~134, no.~1, pp.~201--240.

\bibitem{sheppard_world-championship-caliber_2002}
B.~Sheppard, ``World-championship-caliber scrabble,'' vol.~134, no.~1, pp.~241
  -- 275.

\bibitem{browne_survey_2012}
C.~B. Browne, E.~Powley, D.~Whitehouse, S.~M. Lucas, P.~I. Cowling,
  P.~Rohlfshagen, S.~Tavener, D.~Perez, S.~Samothrakis, and S.~Colton, ``A
  survey of monte carlo tree search methods,'' vol.~4, no.~1, pp.~1--43.

\bibitem{rosin_multi-armed_2011}
C.~D. Rosin, ``Multi-armed bandits with episode context,'' vol.~61, no.~3,
  pp.~203--230.

\bibitem{bjarnason_lower_2009}
R.~Bjarnason, A.~Fern, and P.~Tadepalli, ``Lower bounding klondike solitaire
  with monte-carlo planning,'' in {\em Proceedings of the Nineteenth
  International Conference on International Conference on Automated Planning
  and Scheduling}, {ICAPS}'09, pp.~26--33, {AAAI} Press.

\bibitem{audibert_minimax_2009}
J.-Y. Audibert and S.~Bubeck, ``Minimax policies for adversarial and stochastic
  bandits,'' pp.~217--226.

\bibitem{sturtevant_analysis_2008}
N.~R. Sturtevant, ``An analysis of {UCT} in multi-player games,'' in {\em
  Computers and Games}, Lecture Notes in Computer Science, pp.~37--49,
  Springer, Berlin, Heidelberg.

\bibitem{kulkarni_hierarchical_2016}
T.~D. Kulkarni, K.~Narasimhan, A.~Saeedi, and J.~Tenenbaum, ``Hierarchical deep
  reinforcement learning: Integrating temporal abstraction and intrinsic
  motivation,'' in {\em Advances in Neural Information Processing Systems 29}
  (D.~D. Lee, M.~Sugiyama, U.~V. Luxburg, I.~Guyon, and R.~Garnett, eds.),
  pp.~3675--3683, Curran Associates, Inc.

\bibitem{dietterich_hierarchical_1999}
T.~G. Dietterich, ``Hierarchical reinforcement learning with the {MAXQ} value
  function decomposition,''

\bibitem{anthony_thinking_2017}
T.~Anthony, Z.~Tian, and D.~Barber, ``Thinking fast and slow with deep learning
  and tree search,''

\end{thebibliography}
