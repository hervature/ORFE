
@article{silver_mastering_2016,
	title = {Mastering the game of Go with deep neural networks and tree search},
	volume = {529},
	url = {http://dx.doi.org/10.1038/nature16961},
	pages = {484},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	date = {2016-01-27}
}

@article{silver_mastering_2017,
	title = {Mastering the game of Go without human knowledge},
	volume = {550},
	url = {http://dx.doi.org/10.1038/nature24270},
	pages = {354},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
	date = {2017-10-18}
}

@article{silver_mastering_2017-1,
	title = {Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm},
	url = {http://arxiv.org/abs/1712.01815},
	abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the {AlphaGo} Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single {AlphaZero} algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, {AlphaZero} achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
	journaltitle = {{arXiv}:1712.01815 [cs]},
	author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
	urldate = {2018-01-18},
	date = {2017-12-05},
	eprinttype = {arxiv},
	eprint = {1712.01815},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning},
	file = {arXiv\:1712.01815 PDF:/home/zachary/Zotero/storage/QEPH69RY/Silver et al. - 2017 - Mastering Chess and Shogi by Self-Play with a Gene.pdf:application/pdf;arXiv.org Snapshot:/home/zachary/Zotero/storage/5UWXHDRH/1712.html:text/html}
}

@inreference{tromp_sequence_2016,
	title = {Sequence A094777},
	booktitle = {The On-Line Encyclopedia of Integer Sequences},
	author = {Tromp, John},
	date = {2016}
}

@inproceedings{kocsis_bandit_2006,
	location = {Berlin, Heidelberg},
	title = {Bandit Based Monte-carlo Planning},
	isbn = {978-3-540-45375-8},
	url = {http://dx.doi.org/10.1007/11871842_29},
	doi = {10.1007/11871842_29},
	series = {{ECML}'06},
	abstract = {For large state-space Markovian Decision Problems Monte-Carlo planning is one of the few viable approaches to find near-optimal solutions. In this paper we introduce a new algorithm, {UCT}, that applies bandit ideas to guide Monte-Carlo planning. In finite-horizon or discounted {MDPs} the algorithm is shown to be consistent and finite sample bounds are derived on the estimation error due to sampling. Experimental results show that in several domains, {UCT} is significantly more efficient than its alternatives.},
	pages = {282--293},
	booktitle = {Proceedings of the 17th European Conference on Machine Learning},
	publisher = {Springer-Verlag},
	author = {Kocsis, Levente and Szepesvári, Csaba},
	urldate = {2018-01-18},
	date = {2006}
}

@article{auer_finite-time_2002,
	title = {Finite-time Analysis of the Multiarmed Bandit Problem},
	volume = {47},
	issn = {0885-6125, 1573-0565},
	url = {https://link.springer.com/article/10.1023/A:1013689704352},
	doi = {10.1023/A:1013689704352},
	abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.},
	pages = {235--256},
	number = {2},
	journaltitle = {Machine Learning},
	shortjournal = {Machine Learning},
	author = {Auer, Peter and Cesa-Bianchi, Nicolò and Fischer, Paul},
	urldate = {2018-01-18},
	date = {2002-05-01},
	langid = {english},
	file = {Full Text PDF:/home/zachary/Zotero/storage/9VMKCIYL/Auer et al. - 2002 - Finite-time Analysis of the Multiarmed Bandit Prob.pdf:application/pdf;Snapshot:/home/zachary/Zotero/storage/QXY8FEJK/A1013689704352.html:text/html}
}

@article{rosin_multi-armed_2011,
	title = {Multi-armed bandits with episode context},
	volume = {61},
	issn = {1012-2443, 1573-7470},
	url = {https://link.springer.com/article/10.1007/s10472-011-9258-6},
	doi = {10.1007/s10472-011-9258-6},
	abstract = {A multi-armed bandit episode consists of n trials, each allowing selection of one of K arms, resulting in payoff from a distribution over [0,1] associated with that arm. We assume contextual side information is available at the start of the episode. This context enables an arm predictor to identify possible favorable arms, but predictions may be imperfect so that they need to be combined with further exploration during the episode. Our setting is an alternative to classical multi-armed bandits which provide no contextual side information, and is also an alternative to contextual bandits which provide new context each individual trial. Multi-armed bandits with episode context can arise naturally, for example in computer Go where context is used to bias move decisions made by a multi-armed bandit algorithm. The {UCB}1 algorithm for multi-armed bandits achieves worst-case regret bounded by O(Knlog(n)‾‾‾‾‾‾‾‾‾√)O(Knlog⁡(n))O{\textbackslash}left({\textbackslash}sqrt\{Kn{\textbackslash}log(n)\}{\textbackslash}right). We seek to improve this using episode context, particularly in the case where K is large. Using a predictor that places weight M i {\textgreater} 0 on arm i with weights summing to 1, we present the {PUCB} algorithm which achieves regret O(1M∗nlog(n)‾‾‾‾‾‾‾√)O(1M∗nlog⁡(n))O{\textbackslash}left({\textbackslash}frac\{1\}\{M\_\{{\textbackslash}ast\}\}{\textbackslash}sqrt\{n{\textbackslash}log(n)\}{\textbackslash}right) where M ∗ is the weight on the optimal arm. We illustrate the behavior of {PUCB} with small simulation experiments, present extensions that provide additional capabilities for {PUCB}, and describe methods for obtaining suitable predictors for use with {PUCB}.},
	pages = {203--230},
	number = {3},
	journaltitle = {Annals of Mathematics and Artificial Intelligence},
	shortjournal = {Ann Math Artif Intell},
	author = {Rosin, Christopher D.},
	urldate = {2018-01-18},
	date = {2011-03-01},
	langid = {english},
	file = {Snapshot:/home/zachary/Zotero/storage/FMJWJDCJ/s10472-011-9258-6.html:text/html}
}

@incollection{tesauro_-line_1997,
	title = {On-line Policy Improvement using Monte-Carlo Search},
	url = {http://papers.nips.cc/paper/1302-on-line-policy-improvement-using-monte-carlo-search.pdf},
	pages = {1068--1074},
	booktitle = {Advances in Neural Information Processing Systems 9},
	publisher = {{MIT} Press},
	author = {Tesauro, Gerald and Galperin, Gregory R.},
	editor = {Mozer, M. C. and Jordan, M. I. and Petsche, T.},
	urldate = {2018-01-18},
	date = {1997},
	file = {NIPS Full Text PDF:/home/zachary/Zotero/storage/XSAZCY6K/Tesauro and Galperin - 1997 - On-line Policy Improvement using Monte-Carlo Searc.pdf:application/pdf;NIPS Snapshort:/home/zachary/Zotero/storage/TEWA8AAE/1302-on-line-policy-improvement-using-monte-carlo-search.html:text/html}
}

@article{sheppard_world-championship-caliber_2002,
	title = {World-championship-caliber Scrabble},
	volume = {134},
	issn = {0004-3702},
	url = {http://www.sciencedirect.com/science/article/pii/S0004370201001667},
	doi = {https://doi.org/10.1016/S0004-3702(01)00166-7},
	pages = {241 -- 275},
	number = {1},
	journaltitle = {Artificial Intelligence},
	author = {Sheppard, Brian},
	date = {2002},
	keywords = {B, Dictionary representations, Heuristic search, Probability-weighted search, Scrabble, Simulations}
}

@article{billings_challenge_2002,
	title = {The challenge of poker},
	volume = {134},
	issn = {0004-3702},
	url = {http://www.sciencedirect.com/science/article/pii/S0004370201001308},
	doi = {10.1016/S0004-3702(01)00130-8},
	pages = {201--240},
	number = {1},
	journaltitle = {Artificial Intelligence},
	shortjournal = {Artificial Intelligence},
	author = {Billings, Darse and Davidson, Aaron and Schaeffer, Jonathan and Szafron, Duane},
	date = {2002-01-01},
	keywords = {Simulations, Computer poker, Imperfect information, Neural networks, Opponent modeling}
}

@article{browne_survey_2012,
	title = {A Survey of Monte Carlo Tree Search Methods},
	volume = {4},
	issn = {1943-068X},
	doi = {10.1109/TCIAIG.2012.2186810},
	abstract = {Monte Carlo tree search ({MCTS}) is a recently proposed search method that combines the precision of tree search with the generality of random sampling. It has received considerable interest due to its spectacular success in the difficult problem of computer Go, but has also proved beneficial in a range of other domains. This paper is a survey of the literature to date, intended to provide a snapshot of the state of the art after the first five years of {MCTS} research. We outline the core algorithm's derivation, impart some structure on the many variations and enhancements that have been proposed, and summarize the results from the key game and nongame domains to which {MCTS} methods have been applied. A number of open research questions indicate that the field is ripe for future work.},
	pages = {1--43},
	number = {1},
	journaltitle = {{IEEE} Transactions on Computational Intelligence and {AI} in Games},
	author = {Browne, C. B. and Powley, E. and Whitehouse, D. and Lucas, S. M. and Cowling, P. I. and Rohlfshagen, P. and Tavener, S. and Perez, D. and Samothrakis, S. and Colton, S.},
	date = {2012-03},
	keywords = {Artificial intelligence, Artificial intelligence ({AI}), bandit-based methods, computer Go, Computers, Decision theory, game search, game theory, Game theory, Games, key game, Markov processes, {MCTS} research, Monte Carlo methods, Monte Carlo tree search ({MCTS}), Monte carlo tree search methods, nongame domains, random sampling generality, tree searching, upper confidence bounds ({UCB}), upper confidence bounds for trees ({UCT})},
	file = {IEEE Xplore Abstract Record:/home/zachary/Zotero/storage/BB4EHQ6N/6145622.html:text/html}
}

@inproceedings{bjarnason_lower_2009,
	location = {Thessaloniki, Greece},
	title = {Lower Bounding Klondike Solitaire with Monte-Carlo Planning},
	isbn = {978-1-57735-406-2},
	url = {http://dl.acm.org/citation.cfm?id=3037223.3037228},
	series = {{ICAPS}'09},
	abstract = {Despite its ubiquitous presence, very little is known about the odds of winning the simple card game of Klondike Solitaire. The main goal of this paper is to investigate the use of probabilistic planning to shed light on this issue. Unfortunatley, most probabilistic planning techniques are not well suited for Klondike due to the difficulties of representing the domain in standard planning languages and the complexity of the required search. Klondike thus serves as an interesting addition to the complement of probabilistic planning domains. In this paper, we study Klondike using several sampling-based planning approaches including {UCT}, hindsight optimization, and sparse sampling, and establish empirical lower bounds on their performance. We also introduce novel combinations of these approaches and evaluate them in Klondike. We provide a theoretical bound on the sample complexity of a method that naturally combines sparse sampling and {UCT}. Our results demonstrate that there is a policy that within tight confidence intervals wins over 35\% of Klondike games. This result is the first reported empirical lower bound of an optimal Klondike policy.},
	pages = {26--33},
	booktitle = {Proceedings of the Nineteenth International Conference on International Conference on Automated Planning and Scheduling},
	publisher = {{AAAI} Press},
	author = {Bjarnason, Ronald and Fern, Alan and Tadepalli, Prasad},
	urldate = {2018-01-19},
	date = {2009}
}

@inproceedings{audibert_minimax_2009,
	title = {Minimax policies for adversarial and stochastic bandits},
	url = {https://hal-enpc.archives-ouvertes.fr/hal-00834882/document},
	abstract = {We fill in a long open gap in the characterization of the minimax rate for the multi-armed bandit prob- lem. Concretely, we remove an extraneous loga- rithmic factor in the previously known upper bound and propose a new family of randomized algorithms based on an implicit normalization, as well as a new analysis. We also consider the stochastic case, and prove that an appropriate modification of the upper confidence bound policy {UCB}1 (Auer et al., 2002) achieves the distribution-free optimal rate while still having a distribution-dependent rate log- arithmic in the number of plays.},
	eventtitle = {{COLT}},
	pages = {217--226},
	author = {Audibert, Jean-Yves and Bubeck, Sébastien},
	urldate = {2018-01-19},
	date = {2009-06-18},
	langid = {english},
	file = {Full Text PDF:/home/zachary/Zotero/storage/K3QNWP6Y/Audibert and Bubeck - 2009 - Minimax policies for adversarial and stochastic ba.pdf:application/pdf;Snapshot:/home/zachary/Zotero/storage/VARAKAG9/hal-00834882.html:text/html}
}

@inproceedings{sturtevant_analysis_2008,
	title = {An Analysis of {UCT} in Multi-player Games},
	isbn = {978-3-540-87607-6 978-3-540-87608-3},
	url = {https://link.springer.com/chapter/10.1007/978-3-540-87608-3_4},
	doi = {10.1007/978-3-540-87608-3_4},
	series = {Lecture Notes in Computer Science},
	abstract = {The {UCT} algorithm has been exceedingly popular for Go, a two-player game, significantly increasing the playing strength of Go programs in a very short time. This paper provides an analysis of the {UCT} algorithm in multi-player games, showing that {UCT}, when run in a multi-player game, is computing a mixed-strategy equilibrium, as opposed to maxn, which computes a pure-strategy equilibrium. We analyze the performance of {UCT} in several known domains and show that it performs as well or better than existing algorithms.},
	eventtitle = {International Conference on Computers and Games},
	pages = {37--49},
	booktitle = {Computers and Games},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Sturtevant, Nathan R.},
	urldate = {2018-01-19},
	date = {2008-09-29},
	langid = {english},
	file = {Snapshot:/home/zachary/Zotero/storage/IXAEPASS/978-3-540-87608-3_4.html:text/html}
}

@incollection{kulkarni_hierarchical_2016,
	title = {Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation},
	url = {http://papers.nips.cc/paper/6233-hierarchical-deep-reinforcement-learning-integrating-temporal-abstraction-and-intrinsic-motivation.pdf},
	pages = {3675--3683},
	booktitle = {Advances in Neural Information Processing Systems 29},
	publisher = {Curran Associates, Inc.},
	author = {Kulkarni, Tejas D and Narasimhan, Karthik and Saeedi, Ardavan and Tenenbaum, Josh},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	date = {2016}
}

@article{dietterich_hierarchical_1999,
	title = {Hierarchical Reinforcement Learning with the {MAXQ} Value Function Decomposition},
	url = {http://arxiv.org/abs/cs/9905014},
	abstract = {This paper presents the {MAXQ} approach to hierarchical reinforcement learning based on decomposing the target Markov decision process ({MDP}) into a hierarchy of smaller {MDPs} and decomposing the value function of the target {MDP} into an additive combination of the value functions of the smaller {MDPs}. The paper defines the {MAXQ} hierarchy, proves formal results on its representational power, and establishes five conditions for the safe use of state abstractions. The paper presents an online model-free learning algorithm, {MAXQ}-Q, and proves that it converges wih probability 1 to a kind of locally-optimal policy known as a recursively optimal policy, even in the presence of the five kinds of state abstraction. The paper evaluates the {MAXQ} representation and {MAXQ}-Q through a series of experiments in three domains and shows experimentally that {MAXQ}-Q (with state abstractions) converges to a recursively optimal policy much faster than flat Q learning. The fact that {MAXQ} learns a representation of the value function has an important benefit: it makes it possible to compute and execute an improved, non-hierarchical policy via a procedure similar to the policy improvement step of policy iteration. The paper demonstrates the effectiveness of this non-hierarchical execution experimentally. Finally, the paper concludes with a comparison to related work and a discussion of the design tradeoffs in hierarchical reinforcement learning.},
	journaltitle = {{arXiv}:cs/9905014},
	author = {Dietterich, Thomas G.},
	urldate = {2018-01-19},
	date = {1999-05-21},
	eprinttype = {arxiv},
	eprint = {cs/9905014},
	keywords = {Computer Science - Learning, I.2.6},
	file = {arXiv\:cs/9905014 PDF:/home/zachary/Zotero/storage/N3RWE22V/Dietterich - 1999 - Hierarchical Reinforcement Learning with the MAXQ .pdf:application/pdf;arXiv.org Snapshot:/home/zachary/Zotero/storage/CZMWXNU6/9905014.html:text/html}
}

@article{anthony_thinking_2017,
	title = {Thinking Fast and Slow with Deep Learning and Tree Search},
	url = {http://arxiv.org/abs/1705.08439},
	abstract = {Sequential decision making problems, such as structured prediction, robotic control, and game playing, require a combination of planning policies and generalisation of those plans. In this paper, we present Expert Iteration ({ExIt}), a novel reinforcement learning algorithm which decomposes the problem into separate planning and generalisation tasks. Planning new policies is performed by tree search, while a deep neural network generalises those plans. Subsequently, tree search is improved by using the neural network policy to guide search, increasing the strength of new plans. In contrast, standard deep Reinforcement Learning algorithms rely on a neural network not only to generalise plans, but to discover them too. We show that {ExIt} outperforms {REINFORCE} for training a neural network to play the board game Hex, and our final tree search agent, trained tabula rasa, defeats {MoHex} 1.0, the most recent Olympiad Champion player to be publicly released.},
	journaltitle = {{arXiv}:1705.08439 [cs]},
	author = {Anthony, Thomas and Tian, Zheng and Barber, David},
	urldate = {2018-01-19},
	date = {2017-05-23},
	eprinttype = {arxiv},
	eprint = {1705.08439},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv\:1705.08439 PDF:/home/zachary/Zotero/storage/QC9UQ3Y2/Anthony et al. - 2017 - Thinking Fast and Slow with Deep Learning and Tree.pdf:application/pdf;arXiv.org Snapshot:/home/zachary/Zotero/storage/3S7FTWF9/1705.html:text/html}
}