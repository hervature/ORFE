\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{datetime}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage[english]{babel}
\usepackage{matlab-prettifier}
\usepackage{graphicx}
\usepackage[makeroom]{cancel}
\usepackage{afterpage}
\usepackage{capt-of}
\usepackage{bm}
\usepackage{float}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\newtheoremstyle{colon}{\topsep}{\topsep}{}{}{\bfseries}{:}{ }{}
\theoremstyle{colon}
\newtheorem{exercise}{Exercise}
\newtheorem*{answer}{Answer}

\title{ELE 535: Machine Learning and Pattern Recognition \\ Homework 6}
\author{Zachary Hervieux-Moore}

\newdate{date}{14}{11}{2018}
\date{\displaydate{date}}

\begin{document}

\maketitle

\clearpage

\begin{exercise}
  The density $f_X(x)$ takes the exponential form
  \begin{gather*}
    f_X(x) = \frac{h(x)}{Z(\theta_0)} e^{\langle \theta_0, t(x) \rangle}
  \end{gather*}

  You want to use data from $m$ i.i.d. draws from $f_X$ to estimate the value of $\theta_0$. Given the training data, the likelihood function for any parameter value $\theta$ is

  \begin{gather*}
    L(\theta) = \prod_{i=1}^m \frac{1}{Z(\theta)} h(x_i) e^{\langle \theta, t(x_i) \rangle}
  \end{gather*}

  The maximum likelihood estimate $\hat{\theta}_0$ of $\theta_0$ is obtained by solving

  \begin{gather*}
    \hat{\theta}_0 = \argmax_\theta L(\theta)
  \end{gather*}

  Show that $\hat{\theta}_0$ must satisfy

  \begin{gather*}
    \nabla \ln (Z(\hat{\theta}_0)) = \frac{1}{m} \sum_{i=1}^m t(x_i)
  \end{gather*}
\end{exercise}

\begin{answer}

\end{answer}

\clearpage

\begin{exercise}
  Given $m$ i.i.d. draws from a multivariate Gaussian density, use the method in (Q1) to find the maximum likelihood estimates of the mean $\mu$ and covariance matrix $\Sigma$ of the density.
\end{exercise}

\begin{answer}

\end{answer}

\clearpage

\begin{exercise}
  \textbf{Empirical statistics, MSE affine prediction, and least squares.} Fix a training dataset $\{(x_i, y_i\}_{i=1}^m$, with examples $x_i \in \mathbb{R}^n$ and targets $y_i \in \mathbb{R}^q$. Let $X$ denote the matrix with the examples as its columns, $Y$ denote the matrix with the corresponding targets as its columns. Define the following first and second order empirical statistics of the data:

  \begin{gather}
    \label{eq:3.1}
    \hat{\mu}_X = \frac{1}{m} X \bm{1}_m \quad \hat{\mu}_Y = \frac{1}{m} Y \bm{1}_m \\
    \hat{\Sigma}_X = \frac{1}{m} (X - \hat{\mu}_X \bm{1}_m^T)(X - \hat{\mu}_X \bm{1}_m^T)^T \quad \hat{\Sigma}_{XY} = \frac{1}{m} (X - \hat{\mu}_X \bm{1}_m^T)(Y - \hat{\mu}_Y \bm{1}_m^T)^T \nonumber
  \end{gather}

  An optimal MSE affine estimator $\hat{y}(x = W^T x + b$ based on the empirical statistics in (\ref{eq:3.1}) must satisfy

  \begin{gather}
    \label{eq:3.2}
    \hat{\Sigma}_X W = \hat{\Sigma}_{XY} \quad b = \hat{\mu}_Y - W^T \hat{\mu}_X
  \end{gather}

  \begin{enumerate}[label=\alph*)]
    \item Consider the least squares problem
      \begin{gather}
        \label{eq:3.3}
        W_*, b_* = \argmin_{W \in \mathbb{R}^{n \times q}, b \in \mathbb{R}^q} \lVert Y - W^T X - b \bm{1}_m^T \rVert_F^2
      \end{gather}
      Show that $W_*, b_*$ satisfies (\ref{eq:3.2}). Thus directly solving the least squares problem (\ref{eq:3.3}) yields an optimal MSE affine estimator for the empirical first and second order statistics in (\ref{eq:3.1}).

    \item Consider the ridge regression problem
      \begin{gather*}
        W_{r*}, b_{r*} = \argmin_{W \in \mathbb{R}^{n \times q}, b \in \mathbb{R}^q} \frac{1}{m} \lVert Y - W^T X - b \bm{1}_m^T \rVert_F^2 + \lambda \lVert W \rVert_F^2, \quad \lambda > 0
      \end{gather*}
      Determine if $W_{r*}, b_{r*}$ satisfy (\ref{eq:3.2}). If not, what needs to be changd in (\ref{eq:3.1}) to ensure $W_{r*}, b_{r*}$ satisfy (\ref{eq:3.2}). Interpret the change you suggest.
  \end{enumerate}
\end{exercise}

\begin{answer}

\end{answer}

\clearpage

\begin{exercise}
  \textbf{Bias, error covariance, and MSE.} Consider random vectors $X$ and $Y$ with a joint density $f_{XY}$ and PD covariance $\Sigma$. Let $X$ have mean $\mu_X \in \mathbb{R}^n$ and covariance $\Sigma_X \in \mathbb{R}^{n \times n}$, $Y$ have $\mu_Y \in \mathbb{R}^q$ and covariance $\Sigma_Y \in \mathbb{R}^{q \times q}$, and let the cross-covariance of $X$ and $Y$ be $\Sigma_{XY} \in \mathbb{R}^{n \times q}$.

  Let $\hat{y}(x)$ be an estimator of $Y$ given $X = x$, and denote the corresponding prediction error by $E \triangleq Y - \hat{y}(X)$. Of interest is $\mu_E$, $\Sigma_E$ and the MSE. The estimator is said to be \textit{unbiased} if $\mu_E = \bm{0}$.

  \begin{enumerate}[label=\alph*)]
    \item For any estimator $\hat{y}$ with finite $\mu_E$ and MSE, show that MSE($\hat{y}$) = trace($\Sigma_E$) + $\lVert \mu_E \rVert_2^2$. This shows that the MSE is the sum of two termsLvthe total variance trace($\Sigma_E$) of the error, and the squared norm of the bias $\lVert \mu_E \rVert_2^2$.

    \item Let $\hat{y}(x) = \mu_Y$. Show that this is an unbiased estimator, determine $\Sigma_E$, show that $\Sigma_E$ is PD, and determine the estimator MSE.

    \item The MMSE affine estimator of $Y$ given $X = x$ is
      \begin{gather*}
        \hat{y}^*(x) = \mu_Y + {W^*}^T (x - \mu_X) \quad \text{ with } \Sigma_X W^* = \Sigma_{XY}
      \end{gather*}
      Show that $\hat{y}^*(\cdot)$ is an unbiased estimator, determine $\Sigma_E$, show that $\Sigma_E$ is PD, and determine the estimator MSE.
  \end{enumerate}
\end{exercise}

\begin{answer}

\end{answer}

\clearpage

\begin{exercise}
  \textbf{The derivative and gradient of $\lVert M \rVert_2$.} For $M \in \mathbb{R}^{m \times n}$ define $f(M) = \lVert M \rVert_2$. Determine a sufficient condition for the derivative of $f$ to exist at $M$, and under these conditions find $D f(M)(H)$ and $\nabla f(M)$.
\end{exercise}

\begin{answer}

\end{answer}

\end{document}