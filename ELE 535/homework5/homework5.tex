\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{datetime}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage[english]{babel}
\usepackage{matlab-prettifier}
\usepackage{graphicx}
\usepackage[makeroom]{cancel}
\usepackage{afterpage}
\usepackage{capt-of}
\usepackage{bm}
\usepackage{float}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\newtheoremstyle{colon}{\topsep}{\topsep}{}{}{\bfseries}{:}{ }{}
\theoremstyle{colon}
\newtheorem{exercise}{Exercise}
\newtheorem*{answer}{Answer}

\title{ELE 535: Machine Learning and Pattern Recognition \\ Homework 5}
\author{Zachary Hervieux-Moore}

\newdate{date}{22}{10}{2018}
\date{\displaydate{date}}

\begin{document}

\maketitle

\clearpage

\begin{exercise}
  Derive the derivative, and if exists the gradient, of the following functions.

  \begin{enumerate}[label=\alph*)]
    \item For $x \in \mathbb{R}^n$, $f(x) = \sum_{j=1}^n x_j$.
    \item For $x \in \mathbb{R}^n$, $f(x) = e^{\sum_{j=1}^n x_j}$.
    \item For $x \in \mathbb{R}^n$, $f(x) = x^T A x + a^T x + b$, where $b \in \mathbb{R}$, $a \in \mathbb{R}^n$, and $A \in \mathbb{R}^{n \times n}$.
    \item For $M \in \mathbb{R}^{n \times n}$, $f(M) = \lVert M \rVert_F^2$.
    \item For $x \in \mathbb{R}^n$, $f(x) = x x^T \in \mathbb{R}^{n \times n}$.
  \end{enumerate}
\end{exercise}

\begin{answer}
  
\end{answer}

\clearpage

\begin{exercise}
  \textbf{Matrix Inversion Lemma.} Let $M = \begin{bmatrix} A & B \\ C & D \end{bmatrix}$ with $A \in \mathbb{R}^{p \times p}$, $D \in \mathbb{R}^{q \times q}$, $B \in \mathbb{R}^{p \times q}$, $C \in \mathbb{R}^{q \times p}$.

  \begin{enumerate}[label=\alph*)]
    \item If $A$ and $D$, and at least one of $S_A$ or $S_D$ are invertible, derive the equality (this was partially done in class):
      \begin{gather*}
        (A - B D^{-1} C)^{-1} = A^{-1} + A^{-1} B (D - C A^{-1} B)^{-1} C A^{-1}
      \end{gather*}

    \item Use part a) to show that if $A$ and $D$, and at least one of $A + B D C$ or $D^{-1} + C A^{-1} B$ are invertible, then:
      \begin{gather*}
        (A + B D C)^{-1} = A^{-1} - A^{-1} B (D^{-1} + C A^{-1} B)^{-1} C A^{-1}
      \end{gather*}
  \end{enumerate}
\end{exercise}

\begin{answer}
  
\end{answer}

\clearpage

\begin{exercise}
  \textbf{On-line least squares with mini-batch updates.} You want to solve a least squares regression problem by processing the data in small batches (mini-batches), yielding a new least squares solution after each update. assume each mini-batch contains $k$ training examples. Group the examples in the $t^{th}$ mini-batch into the columns of $X_t \in \mathbb{R}^{n \times k}$, and the corresponding targets into the rows of $y_t \in \mathbb{R}^k$. Let $P_{t-1} = \sum_{i=1}^{t-1} X_i X_i^T \in \mathbb{R}^{n \times n}$. Assume $P_{t-1}^{-1}$ exists and is known. Similarly, let $s_{t-1} = \sum_{i=1}^{t-1} X_i y_i \mathbb{R}^n$. Derive the following equations for the $t^{th}$ mini-batch update:

  \begin{gather*}
    \hat{y}_t \triangleq X_t^T w_{t-1}^* \text{ target prediction} \\
    w_t^* = w_{t-1}^* + P_{t-1}^{-1} X_t [I_k + X_t^T P_{t-1}^{-1} X_t]^{-1} (y_t - \hat{y}_t) \text{ update } w^* \\
    P_t^{-1} = P_{t-1}^{-1} - P_{t-1}^{-1} X_t [I_k + X_t^T P_{t-1}^{-1} X_t]^{-1} X_t^T P_{t-1}^{-1} \text{ update } P.
  \end{gather*}

  How do these equations change if the mini-batches are not all the same size?
\end{exercise}

\begin{answer}
  
\end{answer}

\clearpage

\begin{exercise}
  \textbf{Linear regression with vector targets.} We are given training data $\{ (x_i, z_i)_{i=1}^m \}$ with input examples $x_i \in \mathbb{R}^n$ and vector targets $z_i \in \mathbb{R}^d$. Place the input examples into the columns of $X \in \mathbb{R}^{n \times m}$ and the targets into the columns of $Z \in \mathbb{R}^{d \times m}$. We want to learn a linear predictor of the vector targets $z \in \mathbb{R}^d$ of test inputs $x \in \mathbb{R}^n$. To do so, first use the training data to find:

  \begin{gather*}
    W^* = \argmin_{W \mathbb{R}^{n \times d}} \lVert Y - FW \rVert_F^2 + \lambda \lVert W \rVert_F^2,
  \end{gather*}

  where we have set $Y = Z^T$ and $F = X^T$, and we require $\lambda \leq 0$ ($\lambda = 0$ removes the ridge regularizer).

  \begin{enumerate}[label=\alph*)]
    \item Show that the above separates into $d$ standard ridge regression problems, each solvable separately.
    \item Without using the property in a), set the derivative of the objective function w.r.t. $W$ equal to zero, and find an expression for the solution $W^*$. Is the separation property evident from this expression?
  \end{enumerate}
\end{exercise}

\begin{answer}
  
\end{answer}

\clearpage

\begin{exercise}
  \textbf{The softmax function.} This function maps $x \in \mathbb{R}^n$ to a probability mass function $s(x)$ on $n$ outcomes. It can be written as the composition of two functions $s(x) = q(p(x))$, where $p: \mathbb{R}^n \rightarrow \mathbb{R}_+^n$ and $q: \mathbb{R}_+^n \rightarrow_+^n$ are defined by

  \begin{gather*}
    p(x) = [e^{x_i}] \quad q(z) = z/(\bm{1}^T z)
  \end{gather*}

  Here $\mathbb{R}_+^n$ denotes the positive cone $\{ x \in \mathbb{R}^n: x_i > 0 \}$. The function $p(\cdot)$ maps $x \in \mathbb{R}^n$ into the positive cone $\mathbb{R}_+^n$, and for $z \in \mathbb{R}_+^n$, $q(\cdot)$ normalizes $z$ to a probability mass function in $\mathbb{R}_+^n$.

  \begin{enumerate}[label=\alph*)]
    \item Determine the derivative of $p(x)$ at $x$.
    \item Determine the derivative of $q(z)$ at $z$.
    \item Determine the derivative of the softmax function at $x$.
  \end{enumerate}
\end{exercise}

\begin{answer}
  
\end{answer}

\end{document}