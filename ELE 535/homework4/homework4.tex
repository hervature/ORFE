\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{datetime}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage[english]{babel}
\usepackage{matlab-prettifier}
\usepackage{graphicx}
\usepackage[makeroom]{cancel}
\usepackage{afterpage}
\usepackage{capt-of}
\usepackage{bm}
\usepackage{float}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\newtheoremstyle{colon}{\topsep}{\topsep}{}{}{\bfseries}{:}{ }{}
\theoremstyle{colon}
\newtheorem{exercise}{Exercise}
\newtheorem*{answer}{Answer}

\title{ELE 535: Machine Learning and Pattern Recognition \\ Homework 4}
\author{Zachary Hervieux-Moore}

\newdate{date}{15}{10}{2018}
\date{\displaydate{date}}

\begin{document}

\maketitle

\clearpage

\begin{exercise}
  Determine general sufficient conditions (if any exist) under which the indicated function $f$ is convex.

  \begin{enumerate}[label=\alph*)]
    \item $f: \mathbb{R} \rightarrow \mathbb{R}$ with $f(x) = \lvert x \rvert$.
    \item $f: (0, \infty) \rightarrow \mathbb{R}$ with $f(x) = x \ln (x)$.
    \item $f: \mathbb{R}^n \rightarrow \mathbb{R}$ with $f(x) = (x^T Q x)^3$. Here $Q \in \mathbb{R}^{n \times n}$ is symmetric PSD.
    \item $f: \mathbb{R}^n \rightarrow \mathbb{R}$ with $f(x) = 1 + e^{\sum_{i=1}^n \lvert x_i \rvert^3}$.
    \item For $x \in \mathcal{C} = \{ x \in \mathbb{R}^n: x_i > 0, i = 1, \mathellipsis, n \}$ let $\ln(x) = [\ln(x_i)] \in \mathbb{R}^n$ and define $f(x) = x^T \ln(x)$
  \end{enumerate}
\end{exercise}

\begin{answer}
  \

  \begin{enumerate}[label=\alph*)]
    \item Simply applying the triangle inequality we get
      \begin{gather*}
        f(\lambda x + (1-\lambda)y) = \lvert \lambda x + (1-\lambda) y \rvert \\
        \leq \lambda \lvert x \rvert + (1-\lambda) \lvert y \rvert = \lambda f(x) + (1-\lambda) f(y)
      \end{gather*}
      Thus, it is convex.

    \item Here, we take the second derivative since it is continuous on the domain,
      \begin{gather*}
        f''(x) = \frac{1}{x} > 0 \quad \forall x \in (0,\infty)
      \end{gather*}
      As this is always positive, then we have that $f$ is strictly convex.

    \item Here, we use theorem 7.3.2e) that states that $h(x) = g(f(x))$ is convex if $g$ is convex and non decreasing on the range of $f$. Here we have $f(x) = x^T Q x$ whose range is $[0, \infty)$ as $Q$ is PSD and $g(x) = x^3$. Thus, on $[0, \infty)$, we have that $g'(x) = 2 x^2 \geq 0$ and $g''(x) = 6 x \geq 0$ which means that $g$ is on decreasing and convex respectively. Thus, we have shown that the original function is convex.

    \item Again, we use the same theorem as in the previous part. First $g(x) = 1 + e^x$ and $f(x) = (\sum_{i=1}^n \lvert x_i \rvert )^3$. The range of $f(x)$ is $[0, \infty)$. We also have $g'(x) = e^x$ and $g''(x) = e^x$ which are both always positive and so non decreasing and convex. This shows that the original function is convex.

    \item We have that $f(x) = x^T \ln(x)$ which is just a short form for
      \begin{gather*}
        [f(x)]_i = x_i \ln(x_i)
      \end{gather*}
      Thus, we have
      \begin{gather*}
        [\nabla f(x)]_i = 1 + \ln(x_i) \\
        [Hf(x)]_{ii} = \frac{1}{x_i} \text{ and } [Hf(x)]_{ij} = 0 \text{ for } i \neq j
      \end{gather*}
      Since the Hessian is diagonal and always positive, then it is positive definite and hence strictly convex.
  \end{enumerate}
\end{answer}

\clearpage

\begin{exercise}
  You want to learn an unknown function $f: [0,1] \rightarrow \mathbb{R}$ using a set of noisy measurements $(x_j, y_j)$, with $y_j = f(x_j) + \epsilon_j$, $j = 1, \mathellipsis, m$. Your plan is to approximate $f(\cdot)$ by a Fourier series on $[0,1]$ with $q \in \mathbb{N}$ terms:

  \begin{gather*}
    f_q(x) = \frac{a_0}{2} + \sum_{k=1}^q a_k \cos(2 \pi k x) + b_k \sin(2 \pi k x).
  \end{gather*}

  To control the smoothness of $f_q(\cdot)$, you also decide to penalize the size of the coefficients $a_k, b_k$ more heavily as $k$ increases.

  \begin{enumerate}[label=\alph*)]
    \item Formulate the above problem as a regularized regression problem.
    \item For $q=2$, display the regression matrix, the label vector $y$, and the regularization term.
    \item Comment briefly on how to select $q$.
  \end{enumerate}
\end{exercise}

\begin{answer}
  
\end{answer}

\clearpage

\begin{exercise}
  Let $D \in \mathbb{R}^{n \times n}$ be diagonal with nonnegative diagonal entries and consider the problem:

  \begin{gather*}
    \min_{x \in \mathbb{R}^n} \lVert x - y \rVert_2^2 + \lambda \lVert D x \rVert_2^2
  \end{gather*}

  This problem seeks to best approximate $y \in \mathbb{R}^n$ with a nonuniform penalty for large entries in $x$.

  \begin{enumerate}[label=\alph*)]
    \item Solve this problem using the solution of ridge regression.
    \item Show that the objective function is separable into a sum of decoupled terms. Show that this decomposes the problem into $n$ independent scalar problems.
    \item Finc the solution of each scalar problem.
    \item By putting these scalar solutions together, find and interpret the solution to the original problem.
  \end{enumerate}
\end{exercise}

\begin{answer}
  
\end{answer}

\clearpage

\begin{exercise}
  Let $X \in \mathbb{R}^{n \times m}$ and $y \in \mathbb{R}^{m}$ be given, and $\lambda > 0$. Consider the problem

  \begin{gather*}
    w^* = \argmin_{w \in \mathbb{R}^n} \lVert y - X^T w \rVert_2^2 + \lambda \lVert w \rVert_2^2
  \end{gather*}

  From the notes we know that there exists a unique solution $w^*$ and that $w^* \in \mathcal{R}(X)$. Using the above and these two results, show that $w^* = X (X^TX + \lambda I_m)^{-1} y$.
\end{exercise}

\begin{answer}
  
\end{answer}

\clearpage

\begin{exercise}
  One form of regularized least squares can be posed as:

  \begin{gather*}
    w^* = \argmin_{w \in \mathbb{R}^n} \lVert F w - y \rVert_2^2 + \lambda \lVert G w - g \rVert_2^2
  \end{gather*}

  where $F \in \mathbb{R}^{m \times n}$, $y \in \mathbb{R}^m$, $G \in \mathbb{R}^{k \times n}$, $g \in \mathbb{R}^k$, and $\lambda > 0$.

  \begin{enumerate}[label=\alph*)]
    \item Show that a sufficient condition for the above to have a unique solution is that rank($G$) = $n$.
    \item Show that a necessary and sufficient condition is that $\mathcal{N}(F) \cap \mathcal{N}(G) = \mathbf{0}$.
  \end{enumerate}
\end{exercise}

\begin{answer}
  
\end{answer}

\end{document}