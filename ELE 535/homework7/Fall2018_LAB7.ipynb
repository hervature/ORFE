{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELE 435/535 Computational Lab 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be working with the subset of MNIST that was used for HW1 in this HW. The training data contains 10,000 samples of different digits. Let's call it matrix D (of dimension 784 * 10000). The first 1000 columns of D correspond to digit 0 ($D_0$), the next 1000 correspond to digit 1 ($D_1$), etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load('./MNISTcwtrain1000.npy')\n",
    "train_data = train_data.astype(dtype='float64')\n",
    "test_data = np.load('./MNISTcwtest100.npy')\n",
    "test_data = test_data.astype(dtype='float64')\n",
    "\n",
    "train_data = train_data/255.0\n",
    "test_data = test_data/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1) Collaborative Representation Based Classification Using Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In HW4, we used least square regression for classifiying data. Given any new example (x), we would like to represent it as a linear combination of columns of D (hence, the name representation based classification). This can be acheived by finding a vector w (of dimension 1,000) that satisfies: $w = \\arg\\min \\hspace{1mm} \\|Dw - x \\|_2$.\n",
    "\n",
    "The first 100 elements of w ($w_0$) quantify how much of each column from digit 0 are needed to represent x. Similarly, the next 100 elements ($w_1$) correspond to weights on $D_1$, etc.\n",
    "\n",
    "Next, prediction of pixel values of any test image (x) based only on examples of a particual digit $i$ can be found using $y_i' = D_i \\times w_i$. Then, k-th digit that yields the lowest mean squared prediction error (i.e., $k = \\arg\\min \\hspace{1mm} \\|y -y'_i \\|_2$) will determine the label of x. \n",
    "\n",
    "Following this procedure to predict the labels of each test example, the testing accuracy is 0.76."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * Use reduced training set ($Xr$) and test set ($test$) defined below. This will save the running time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xr=np.zeros((784,1000))\n",
    "test=np.zeros((784,100))\n",
    "for ind in range(10):\n",
    "    Xr[:,100*ind:100*(ind+1)] = train_data[:,1000*ind:1000*ind+100]\n",
    "    test[:,10*ind:10*(ind+1)] = test_data[:,100*ind:100*ind+10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuacy :::::: 0.76\n"
     ]
    }
   ],
   "source": [
    "# least square\n",
    "X = np.matrix(Xr)\n",
    "[U,sigma,V] = np.linalg.svd( X, full_matrices=False)\n",
    "index = np.where(sigma>1e-4)\n",
    "trunc = index[0][-1]\n",
    "INV_Mat = np.linalg.pinv(np.dot(X.T,X))\n",
    "Projection_Mat = INV_Mat * (X.T) \n",
    "predicted_label = np.zeros((100,))\n",
    "for i in range(0,100):\n",
    "    test_ex = np.matrix(test[:,i]).T\n",
    "    p = Projection_Mat * test_ex\n",
    "    dist = np.zeros((10,))\n",
    "    for j in range(0,10):\n",
    "        sub_mat = X[:,j*100:(j+1)*100]\n",
    "        sub_W = p[j*100:(j+1)*100]\n",
    "        reconstructed = np.dot(sub_mat, sub_W)\n",
    "        dist[j] = np.linalg.norm(reconstructed - test_ex)\n",
    "    predicted_label[i] = np.argmin(dist)\n",
    "true_label = np.zeros((100,))\n",
    "for i in range(0,10):\n",
    "    true_label[i*10:(i+1)*10] = i\n",
    "test_err = np.count_nonzero(predicted_label - true_label)\n",
    "test_acc = 1-(test_err/100.0)\n",
    "\n",
    "print(\"Test Accuacy :::::: \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of using least square, we use LASSO in this question to find a sparse $w$. The idea behind is that we only want to use a small number of training samples to represent the test sample. Then, the objective is to find a vector $w$ that satisfies: $w = \\arg\\min \\hspace{2mm} \\|Dw - x \\|_2 ^2 + \\lambda \\|w \\|_1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Try $\\lambda = 10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1, 10, 100$ and plot testing accuracy vs. $\\lambda$. Compare the testing accuracy with that from least square. \n",
    "\n",
    "You can use the Lasso solver in Scikit-learn (http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html). Note that the objective function of Lasso solver in Scikit-learn might be different from what we defined here. Please adjust your arguments to the solver accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Number of \"samples\" in this case is actually features\n",
    "num_samples = Xr.shape[0]\n",
    "\n",
    "lambdas = [10**x for x in range(-5,3)]\n",
    "accuracies_arr = []\n",
    "non_zeros_arr = []\n",
    "n_k_arr = []\n",
    "\n",
    "for l in lambdas:\n",
    "    alpha = l/(2.0*num_samples)\n",
    "    clf = linear_model.Lasso(alpha=alpha)\n",
    "    \n",
    "    predicted_label = np.zeros((100,))\n",
    "    non_zeros = 0\n",
    "    n_k = 0\n",
    "        \n",
    "    for i in range(0,100):\n",
    "        test_ex = np.matrix(test[:,i]).T\n",
    "        dist = np.zeros((10,))\n",
    "        \n",
    "        clf.fit(Xr, test_ex)\n",
    "        p = clf.coef_\n",
    "        non_zeros += np.count_nonzero(p)\n",
    "        k = i // 10\n",
    "        \n",
    "        if np.count_nonzero(p) > 0:\n",
    "            n_k += np.count_nonzero(p[k*100:(k+1)*100])/np.count_nonzero(p)\n",
    "        else:\n",
    "            n_k = 1\n",
    "        \n",
    "        for j in range(0,10):\n",
    "            sub_mat = X[:,j*100:(j+1)*100]\n",
    "            sub_W = p[j*100:(j+1)*100]\n",
    "            reconstructed = np.dot(sub_mat, sub_W)\n",
    "            dist[j] = np.linalg.norm(reconstructed - test_ex)\n",
    "            \n",
    "        predicted_label[i] = np.argmin(dist)\n",
    "        \n",
    "    true_label = np.zeros((100,))\n",
    "    \n",
    "    for i in range(0,10):\n",
    "        true_label[i*10:(i+1)*10] = i\n",
    "        \n",
    "    test_err = np.count_nonzero(predicted_label - true_label)\n",
    "    test_acc = 1-(test_err/100.0)\n",
    "\n",
    "    accuracies_arr.append(test_acc)\n",
    "    non_zeros_arr.append(non_zeros/100)\n",
    "    n_k_arr.append(n_k/100)\n",
    "    \n",
    "plt.figure()\n",
    "plt.scatter(lambdas, accuracies_arr)\n",
    "ax = plt.gca()\n",
    "ax.set_xscale('log')\n",
    "plt.title('Test Accuracy vs. Lambda')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Plot the average number of nonzero entries in $w$ vs. $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(lambdas, non_zeros_arr)\n",
    "ax = plt.gca()\n",
    "ax.set_xscale('log')\n",
    "plt.title('Number of Non Zeros vs. Lambda')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.For each test sample, suppose $k$ is the right label, define $nk$ as nonzeros in $w_k$ / total # nonzeros in $w$. Plot the average of $nk$ over all testing samples vs. $\\lambda$. What do you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(lambdas, n_k_arr)\n",
    "ax = plt.gca()\n",
    "ax.set_xscale('log')\n",
    "plt.title('N_k vs. Lambda')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2) Collaborative Representation Based Classification Using Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the elastic net objective function to find $w$: $w = \\arg\\min \\hspace{2mm} \\|Dw - x \\|_2 ^2 + \\lambda (\\alpha \\|w \\|_1+0.5(1-\\alpha) \\|w \\|_2 ^2 )$. It linearly combines the $L1$ and $L2$ penalties. Choose an appropriate $\\lambda$ from Q1, and vary $\\alpha$ in the range $(0,1)$.\n",
    "\n",
    "You can use Scikit-learn ElasticNet (http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html). Refer to the documentation on the web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Plot testing accuracy vs. $\\alpha$. Compare the testing accuracy with that from least square and Lasso. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.Plot the average number of nonzero entries in $w$ vs. $\\alpha$ and average $nk$ vs. $\\alpha$. What do you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3) Orthogonal Matching Pursuit (OMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general sparse least squares problem can be posed as below.\n",
    "\\begin{equation}\n",
    "\\min_{x\\in\\mathbb{R}^n} \\|y-Ax\\|_2^2\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\mbox{s.t.} \\|x\\|_0 \\leq k \n",
    "\\end{equation}\n",
    "\n",
    "Orthogonal Matching Pursuit (OMP) is a greedy algorithm for sparse least squares problem above. Here, we are going to use OMP to find a sparse solution for a synthetic dataset. The dataset is defined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.datasets import make_sparse_coded_signal\n",
    "\n",
    "y, X, w = make_sparse_coded_signal(n_samples=1,n_components=512,n_features=100,n_nonzero_coefs=15,random_state=0)\n",
    "print 'X (train) : ' + str(X.shape[0]) + ' x ' + str(X.shape[1])\n",
    "print 'y (test) : ' + str(y.shape[0]) + ' x 1'\n",
    "print 'w (weight) : ' + str(w.shape[0]) + ' x 1'\n",
    "\n",
    "idx, = w.nonzero()\n",
    "\n",
    "plt.figure()\n",
    "plt.stem(idx, w[idx])\n",
    "plt.xlim([0,512])\n",
    "plt.title('Sparse weight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.Use the orthogonal matching pursuit algorithm to find the $\\hat{w}$ (coefficients) and compare it with the $w$ above (compute $\\|w-\\hat{w}\\|_2^2$). (http://scikit-learn.org/stable/auto_examples/linear_model/plot_omp.html)\n",
    "\n",
    "You can use scikit-learn OrthogonalMatchingPursuit (http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.OrthogonalMatchingPursuit.html). Set $\\mbox{n_nonzero_coefs}=20$, $\\mbox{fit_intercept}=False$ and default for other parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.Implement the OMP function yourself with following requirements. \n",
    "[Termination conditions]\n",
    "\n",
    "1)Number of nonzero elements in $w$. \n",
    "\\begin{equation}\n",
    "\\mbox{number of nonzero elements} = k\n",
    "\\end{equation}\n",
    "2)Tolerance of the residual.\n",
    "\\begin{equation}\n",
    "\\frac{\\|y-\\hat{y}\\|_2}{\\|y\\|_2} \\leq \\mbox{tolerance}\n",
    "\\end{equation}\n",
    "3)Maximum number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function outline\n",
    "def omp(X,y,n_nonzero,tol,max_iter):\n",
    "    pass\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.Repeat question 6 using your OMP function. (Set parameters as $\\mbox{n_nonzero}=15$,$\\mbox{tol}=10^{-30}$,$\\mbox{max_iter}=300$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.Try changing the tolerance value from $10^{-3}$ to $10^{-30}$ in log scale. 1)Plot the error ($\\|w-\\hat{w}\\|_2^2$) vs. tolerance and 2)number of nonzero elements in $\\hat{w}$ vs. tolerance. (Keep $\\mbox{n_nonzero}=15$ and $\\mbox{max_iter}=300$.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
